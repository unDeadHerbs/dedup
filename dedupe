#! /usr/bin/env -S /bin/sh -c "exec \$(dirname \$(realpath "\$0"))/.virtualenv/bin/python -E "\$0" "\$@""
# Forgive the terrible shebang line, I didn't want to have to wrap this in a script, as the program is so short.
import os
import hashlib
import argparse
import re
from tqdm import tqdm

def find_duplicates(folder,pat):
    hash_map = {}
    duplicates = []

    total_files = sum([len(files) for _, _, files in os.walk(folder)])
    pbar = tqdm(total=total_files, desc="Scanning", unit="file")
    patc = re.compile(pat)

    for root, dirs, files in os.walk(folder):
        for file in files:
            file_path = os.path.join(root, file)
            if not patc.search(file_path): continue
            if os.path.isfile(file_path):
                with open(file_path, 'rb') as f:
                    file_hash = hashlib.sha256(f.read()).hexdigest()
                    if file_hash in hash_map:
                        duplicates[hash_map[file_hash]].append(file_path)
                    else:
                        hash_map[file_hash] = len(duplicates)
                        duplicates.append([file_path])
            pbar.update(1)
    pbar.close()
    return duplicates

def filter_files(file_list):
    keep=file_list.copy()
    delete=[]

    def sub_each(pat, sub, s):
        """Generate a list of all possible strings obtained from
        replacing the pattern with the substitution in the input
        string (non-recursive).

        """
        patc=re.compile(pat)
        matches=patc.finditer(s)
        generated=[]
        for match in matches:
            if type(sub)==type(''):
                r=s[:match.start()] + sub + s[match.end():]
            else:
                r=s[:match.start()] + sub(match) + s[match.end():]
            generated.append(r)
        return generated
    
    def pattern_filter(pat,sub):
        # Modifying the list while iterating through it is safe here
        # as the check in the loop body looks at the updated state and
        # the iterators moving though the old list will never look at
        # a deleted element.
        for f in keep:
            patc = re.compile(pat)
            if patc.search(f):
                names_without = sub_each(pat,sub,f)
                for n in names_without:
                    if n in keep:
                        keep.remove(f)
                        delete.append((n,f))
                        break
    
    # If two files have the same name but one contains ' (n)', delete the one containing
    pattern_filter("[0-9]+",'')
    pattern_filter(" *[0-9]+",'')
    pattern_filter("[(][0-9]+[)]",'')
    pattern_filter(" +[(][0-9]+[)]",'')

    # If two files have the same name except for numbers, delete the larger number one (lexicographically later)
    for i in range(1,4):
        pattern_filter("[0-9]+",lambda n:str(int(n.group(0))-i))

    # TODO: if two files have the same name except one has a more specific path, check the list of preferred paths
    # TODO: if one file has a name which is all numbers and another has one with text, delete the one with numbers

    return delete

def delete_files(files):
    """
    Deletes a list of files.

    Args:
    - files (list): A list of file paths to delete.
    """
    pbar=tqdm(total=len(files), desc="Deleting", unit="path")
    for file_path in files:
        try:
            os.remove(file_path)
        except Exception as e:
            print(f"Failed to delete {file_path}: {e}")
        pbar.update(1)
    pbar.close()

def main():
    parser = argparse.ArgumentParser(description='Find and intelligently delete duplicate files in a folder.')
    parser.add_argument('folder', nargs='?', default=os.getcwd(), help='folder to search for duplicates (default: current directory)')
    parser.add_argument('pattern', nargs='?', default='', help='only look at files containing the pattern')
    args = parser.parse_args()

    folder = args.folder
    pat = args.pattern
    duplicates = find_duplicates(folder,pat)
    count_dups = sum([len(d)>1 for d in duplicates])

    pbar = tqdm(total=count_dups, desc="Filtering", unit="path")
    to_delete = []
    for duplicate_group in duplicates:
        if len(duplicate_group)>1:
            delete = filter_files(duplicate_group)
            if len(delete) >= len(duplicate_group):
                print("WARNING: tried to delete everything, skipping")
                print("List of paths was:", delete)
                pbar.update(1)
                continue
            to_delete.extend(delete)
            pbar.update(1)
    pbar.close()
    print("Found",len(to_delete),"file(s) to remove.")
    delete_files([dell for keep,dell in to_delete])
    for f in to_delete:
        print("-",f)


if __name__ == "__main__":
    main()
